{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaperMind ‚Äì Your intelligent assistant for scientific paper discovery and research üöÄüöÄüöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "\n",
    "This project implements an intelligent research assistant that helps users navigate, understand, and analyze scientific literature using LangGraph and advanced language models. By combining various academic APIs with sophisticated paper processing techniques, it creates a seamless experience for researchers, students, and professionals working with academic papers.\n",
    "\n",
    "> NOTE: The presented workflow is not domain specific: each step in the graph can be adapted to a different domain by simply changing the prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Research literature review represents a significant time investment in R&D, with studies showing that researchers spend 30-50% of their time reading, analyzing, and synthesizing academic papers. This challenge is universal across the research community. While thorough literature review is crucial for advancing science and technology, the current process remains inefficient and time-consuming.\n",
    "\n",
    "Key challenges include:\n",
    "- Extensive time commitment (30-50% of R&D hours) dedicated to reading and processing papers\n",
    "- Inefficient search processes across fragmented database ecosystems\n",
    "- Complex task of synthesizing and connecting findings across multiple papers\n",
    "- Resource-intensive maintenance of comprehensive literature reviews\n",
    "- Ongoing effort required to stay current with new publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key components\n",
    "\n",
    " 1. State-Driven Workflow Engine \n",
    "    - StateGraph Architecture: Five-node system for orchestrated research \n",
    "    - Decision Making Node: Query intent analysis and routing \n",
    "    - Planning Node: Research strategy formulation\n",
    "    - Tool Execution Node: Paper retrieval and processing \n",
    "    - Judge Node: Quality validation and improvement cycles \n",
    "\n",
    "2. Paper Processing Integration \n",
    "    - Source Integration, CORE / arXiv API for comprehensive paper access \n",
    "    - Document Processing, PDF content extraction, Text structure preservation \n",
    "\n",
    "3. Analysis Workflow \n",
    "    - State-aware processing pipeline \n",
    "    - Multi-step validation gates \n",
    "    - Quality-focused improvement cycles \n",
    "    - Human-in-the-loop validation options\n",
    "\n",
    "An overview of the workflow is shown below:\n",
    "\n",
    "![image](https://i.ibb.co/0BBzkcb/mermaid-diagram-2024-11-17-195744.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method details\n",
    "\n",
    "1. The system requires \n",
    "    - OpenAI API key to access GPT 4o. This model was chosen after comparing its performance with other, open-source alternatives (in particular Llama 3). However, any other LLM with tool calling capabilities can be used.\n",
    "    - CORE API key for paper retrieval. CORE is one of the larges online repositories for scientific papers, counting over 136 million papers, and offers a free API for personal use. A key can be requested [here](https://core.ac.uk/services/api#form).\n",
    "    - arXiv does not require an API key for accessing its research paper database. arXiv‚Äôs API is open and free to use without registration or authentication.\n",
    "\n",
    "2. Technical Architecture: \n",
    "    - LangGraph for state orchestration.\n",
    "    - PDFplumber for document processing.\n",
    "    - Pydantic for structured data handling.\n",
    "\n",
    "> Acknowledgment: Special thanks to CORE API and arXiv key for enabling academic paper accessüòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This cell installs the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet langchain==0.2.16 langchain-community==0.2.16 langchain-openai==0.1.23 langgraph==0.2.18 langsmith==0.1.114 pdfplumber python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the required libraries and sets the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import urllib3\n",
    "import time\n",
    "\n",
    "import pdfplumber\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, ClassVar, Sequence, TypedDict, Optional\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "load_dotenv()\n",
    "\n",
    "# Load environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "core_api_key = os.getenv(\"CORE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "This cell contains the prompts used in the workflow.\n",
    "\n",
    "The `agent_prompt` contains a section explaining how to use complex queries with the CORE API or arXiv API, enabling the agent to solve more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for the initial decision making on how to reply to the user\n",
    "decision_making_prompt = \"\"\"\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to help the user with their scientific research.\n",
    "\n",
    "Based on the user query, decide if you need to perform a research or if you can answer the question directly.\n",
    "- You should perform a research if the user query requires any supporting evidence or information.\n",
    "- You should answer the question directly only for simple conversational questions, like \"how are you?\".\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to create a step by step plan to answer the user query\n",
    "planning_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher.\n",
    "Your goal is to make a new step by step plan to help the user with their scientific research .\n",
    "\n",
    "Subtasks should not rely on any assumptions or guesses, but only rely on the information provided in the context or look up for any additional information.\n",
    "\n",
    "If any feedback is provided about a previous answer, incorportate it in your new planning.\n",
    "\n",
    "\n",
    "# TOOLS\n",
    "\n",
    "For each subtask, indicate the external tool required to complete the subtask. \n",
    "Tools can be one of the following:\n",
    "{tools}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the agent to answer the user query with the CORE API\n",
    "core_agent_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher. \n",
    "Your goal is to help the user with their scientific research. You have access to a set of external tools to complete your tasks.\n",
    "Follow the plan you wrote to successfully complete the task.\n",
    "\n",
    "Add extensive inline citations to support any claim made in the answer.\n",
    "\n",
    "\n",
    "# EXTERNAL KNOWLEDGE\n",
    "\n",
    "## CORE API\n",
    "\n",
    "The CORE API has a specific query language that allows you to explore a vast papers collection and perform complex queries. See the following table for a list of available operators:\n",
    "\n",
    "| Operator       | Accepted symbols         | Meaning                                                                                      |\n",
    "|---------------|-------------------------|----------------------------------------------------------------------------------------------|\n",
    "| And           | AND, +, space          | Logical binary and.                                                                           |\n",
    "| Or            | OR                     | Logical binary or.                                                                            |\n",
    "| Grouping      | (...)                  | Used to prioritise and group elements of the query.                                           |\n",
    "| Field lookup  | field_name:value       | Used to support lookup of specific fields.                                                    |\n",
    "| Range queries | fieldName(>, <,>=, <=) | For numeric and date fields, it allows to specify a range of valid values to return.         |\n",
    "| Exists queries| _exists_:fieldName     | Allows for complex queries, it returns all the items where the field specified by fieldName is not empty. |\n",
    "\n",
    "Use this table to formulate more complex queries filtering for specific papers, for example publication date/year.\n",
    "Here are the relevant fields of a paper object you can use to filter the results:\n",
    "{\n",
    "  \"authors\": [{\"name\": \"Last Name, First Name\"}],\n",
    "  \"documentType\": \"presentation\" or \"research\" or \"thesis\",\n",
    "  \"publishedDate\": \"2019-08-24T14:15:22Z\",\n",
    "  \"title\": \"Title of the paper\",\n",
    "  \"yearPublished\": \"2019\"\n",
    "}\n",
    "\n",
    "Example queries:\n",
    "- \"machine learning AND yearPublished:2023\"\n",
    "- \"maritime biology AND yearPublished>=2023 AND yearPublished<=2024\"\n",
    "- \"cancer research AND authors:Vaswani, Ashish AND authors:Bello, Irwan\"\n",
    "- \"title:Attention is all you need\"\n",
    "- \"mathematics AND _exists_:abstract\"\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for the agent to answer the user query with the arXiv API\n",
    "arxiv_agent_prompt = \"\"\"\n",
    "# IDENTITY AND PURPOSE\n",
    "\n",
    "You are an experienced scientific researcher.  \n",
    "Your goal is to help the user with their scientific research.  \n",
    "You have access to a set of external tools to complete your tasks.  \n",
    "Follow the plan you wrote to successfully complete the task.  \n",
    "\n",
    "Add extensive inline citations to support any claim made in the answer.  \n",
    "\n",
    "# EXTERNAL KNOWLEDGE\n",
    "\n",
    "## arXiv API  \n",
    "\n",
    "The arXiv API allows you to search for research papers by keywords, authors, and categories.  \n",
    "You can use the following syntax to construct search queries:\n",
    "\n",
    "### **Query Syntax:**\n",
    "- `\"all:{keywords}\"` ‚Üí Searches for `keywords` in **title, abstract, and authors**\n",
    "- `\"ti:{title}\"` ‚Üí Searches for papers with a **specific title**\n",
    "- `\"abs:{abstract_keywords}\"` ‚Üí Searches within the **abstract**\n",
    "- `\"au:{author_name}\"` ‚Üí Searches for a specific **author** (e.g., `\"au:Ashish Vaswani\"`)\n",
    "- `\"cat:{category}\"` ‚Üí Searches by subject **category** (e.g., `\"cat:cs.LG\"` for Machine Learning)\n",
    "- `\"submittedDate:[YYYY-MM-DD TO YYYY-MM-DD]\"` ‚Üí Filters by **submission date range**\n",
    "- **Logical Operators:** `AND`, `OR`, `NOT` can be used for complex queries  \n",
    "- **Grouping:** Parentheses `(...)` can be used to group terms  \n",
    "\n",
    "### **Example Queries:**\n",
    "1. Find recent papers on **LLM agents and reasoning**:  \n",
    "   - `\"all:(LLM AND agent AND reasoning)\"`  \n",
    "2. Find papers on **reinforcement learning by Ashish Vaswani**:  \n",
    "   - `\"all:(reinforcement learning) AND au:Ashish Vaswani\"`  \n",
    "3. Find papers on **neurosymbolic AI submitted after 2024-01-01**:  \n",
    "   - `\"all:(neurosymbolic AI) AND submittedDate:[2024-01-01 TO *]\"`  \n",
    "4. Find **GPT-related** papers in **Machine Learning (cs.LG)**:  \n",
    "   - `\"all:GPT AND cat:cs.LG\"`  \n",
    "\n",
    "Use this format to construct precise queries for retrieving relevant scientific papers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Prompt for the judging step to evaluate the quality of the final answer\n",
    "judge_prompt = \"\"\"\n",
    "You are an expert scientific researcher.\n",
    "Your goal is to review the final answer you provided for a specific user query.\n",
    "\n",
    "Look at the conversation history between you and the user. Based on it, you need to decide if the final answer is satisfactory or not.\n",
    "\n",
    "A good final answer should:\n",
    "- Directly answer the user query. For example, it does not answer a question about a different paper or area of research.\n",
    "- Answer extensively the request from the user.\n",
    "- Take into account any feedback given through the conversation.\n",
    "- Provide inline sources to support any claim made in the answer.\n",
    "\n",
    "In case the answer is not good enough, provide clear and concise feedback on what needs to be improved to pass the evaluation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility classes and functions\n",
    "\n",
    "This cell contains the utility classes and functions used in the workflow. It includes a wrapper around the CORE (arxiv) API, the Pydantic models for the input and output of the nodes, and a few general-purpose functions.\n",
    "\n",
    "The `CoreAPIWrapper` class includes a retry mechanism to handle transient errors and make the workflow more robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreAPIWrapper(BaseModel):\n",
    "    \"\"\"Simple wrapper around the CORE API.\"\"\"\n",
    "    base_url: ClassVar[str] = \"https://api.core.ac.uk/v3\"  \n",
    "    api_key: ClassVar[str] = core_api_key\n",
    "\n",
    "    top_k_results: int = Field(description = \"Top k results obtained by running a query on Core\", default = 1)\n",
    "\n",
    "    def _get_search_response(self, query: str) -> dict:\n",
    "        http = urllib3.PoolManager()\n",
    "\n",
    "        # Retry mechanism to handle transient errors\n",
    "        max_retries = 5    \n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request(\n",
    "                'GET',\n",
    "                f\"{self.base_url}/search/outputs\", \n",
    "                headers={\"Authorization\": f\"Bearer {self.api_key}\"}, \n",
    "                fields={\"q\": query, \"limit\": self.top_k_results}\n",
    "            )\n",
    "            if 200 <= response.status < 300:\n",
    "                return json.loads(response.data.decode('utf-8'))\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx response from CORE API: {response.status} {response.data}\")\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        response = self._get_search_response(query)\n",
    "        results = response.get(\"results\", [])\n",
    "        if not results:\n",
    "            return \"No relevant results were found\"\n",
    "\n",
    "        # Format the results in a string\n",
    "        docs = []\n",
    "        for result in results:\n",
    "            published_date_str = result.get('publishedDate') or result.get('yearPublished', '')\n",
    "            authors_str = ' and '.join([item['name'] for item in result.get('authors', [])])\n",
    "            docs.append((\n",
    "                f\"* ID: {result.get('id', '')},\\n\"\n",
    "                f\"* Title: {result.get('title', '')},\\n\"\n",
    "                f\"* Published Date: {published_date_str},\\n\"\n",
    "                f\"* Authors: {authors_str},\\n\"\n",
    "                f\"* Abstract: {result.get('abstract', '')},\\n\"\n",
    "                f\"* Paper URLs: {result.get('sourceFulltextUrls') or result.get('downloadUrl', '')}\"\n",
    "            ))\n",
    "        return \"\\n-----\\n\".join(docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ArxivAPIWrapper` class to interact with the ArXiv API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAPIWrapper(BaseModel):\n",
    "    \"\"\"Wrapper around the arXiv API for paper searching.\"\"\"\n",
    "    base_url: ClassVar[str] = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    top_k_results: int = Field(description=\"Top k results obtained by running a query on arXiv\", default=1)\n",
    "\n",
    "    def _get_search_response(self, query: str) -> str:\n",
    "        http = urllib3.PoolManager()\n",
    "        search_query = f\"search_query=all:{query}&start=0&max_results={self.top_k_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "\n",
    "        response = http.request(\"GET\", f\"{self.base_url}?{search_query}\")\n",
    "        if response.status != 200:\n",
    "            raise Exception(f\"Failed to fetch data from arXiv: {response.status}\")\n",
    "\n",
    "        return response.data.decode(\"utf-8\")\n",
    "\n",
    "    def _parse_arxiv_response(self, xml_data: str) -> list[dict]:\n",
    "        \"\"\"Parses XML response from arXiv API and extracts relevant paper details.\"\"\"\n",
    "        root = ET.fromstring(xml_data)\n",
    "        namespace = {\"arxiv\": \"http://www.w3.org/2005/Atom\"}\n",
    "\n",
    "        papers = []\n",
    "        for entry in root.findall(\"arxiv:entry\", namespace):\n",
    "            title = entry.find(\"arxiv:title\", namespace).text.strip()\n",
    "            summary = entry.find(\"arxiv:summary\", namespace).text.strip()\n",
    "            published_date = entry.find(\"arxiv:published\", namespace).text.strip()\n",
    "            authors = [author.find(\"arxiv:name\", namespace).text.strip() for author in entry.findall(\"arxiv:author\", namespace)]\n",
    "            link = entry.find(\"arxiv:id\", namespace).text.strip()\n",
    "\n",
    "            papers.append({\n",
    "                \"title\": title,\n",
    "                \"abstract\": summary,\n",
    "                \"published_date\": published_date,\n",
    "                \"authors\": \", \".join(authors),\n",
    "                \"link\": link\n",
    "            })\n",
    "        \n",
    "        return papers\n",
    "\n",
    "    def search(self, query: str) -> str:\n",
    "        xml_response = self._get_search_response(query)\n",
    "        papers = self._parse_arxiv_response(xml_response)\n",
    "\n",
    "        if not papers:\n",
    "            return \"No relevant results were found\"\n",
    "\n",
    "        docs = [\n",
    "            f\"* Title: {paper['title']}\\n\"\n",
    "            f\"* Published Date: {paper['published_date']}\\n\"\n",
    "            f\"* Authors: {paper['authors']}\\n\"\n",
    "            f\"* Abstract: {paper['abstract']}\\n\"\n",
    "            f\"* Link: {paper['link']}\\n\"\n",
    "            for paper in papers\n",
    "        ]\n",
    "        return \"\\n-----\\n\".join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchPapersInput(BaseModel):\n",
    "    \"\"\"Input object to search papers with the CORE API.\"\"\"\n",
    "    query: str = Field(description=\"The query to search for on the selected archive.\")\n",
    "    max_papers: int = Field(description=\"The maximum number of papers to return. It's default to 1, but you can increase it up to 10 in case you need to perform a more comprehensive search.\", default=1, ge=1, le=10)\n",
    "\n",
    "class DecisionMakingOutput(BaseModel):\n",
    "    \"\"\"Output object of the decision making node.\"\"\"\n",
    "    requires_research: bool = Field(description=\"Whether the user query requires research or not.\")\n",
    "    answer: Optional[str] = Field(default=None, description=\"The answer to the user query. It should be None if the user query requires research, otherwise it should be a direct answer to the user query.\")\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    \"\"\"Output object of the judge node.\"\"\"\n",
    "    is_good_answer: bool = Field(description=\"Whether the answer is good or not.\")\n",
    "    feedback: Optional[str] = Field(default=None, description=\"Detailed feedback about why the answer is not good. It should be None if the answer is good.\")\n",
    "\n",
    "def format_tools_description(tools: list[BaseTool]) -> str:\n",
    "    return \"\\n\\n\".join([f\"- {tool.name}: {tool.description}\\n Input arguments: {tool.args}\" for tool in tools])\n",
    "\n",
    "async def print_stream(app: CompiledStateGraph, input: str) -> Optional[BaseMessage]:\n",
    "    display(Markdown(\"## New research running\"))\n",
    "    display(Markdown(f\"### Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(\"### Stream:\\n\\n\"))\n",
    "\n",
    "    # Stream the results \n",
    "    all_messages = []\n",
    "    async for chunk in app.astream({\"messages\": [input]}, stream_mode=\"updates\"):\n",
    "        for updates in chunk.values():\n",
    "            if messages := updates.get(\"messages\"):\n",
    "                all_messages.extend(messages)\n",
    "                for message in messages:\n",
    "                    message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    " \n",
    "    # Return the last message if any\n",
    "    if not all_messages:\n",
    "        return None\n",
    "    return all_messages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent state\n",
    "\n",
    "This cell defines the agent state, which contains the following information:\n",
    "- `requires_research`: Whether the user query requires research or not.\n",
    "- `num_feedback_requests`: The number of times the LLM asked for feedback.\n",
    "- `is_good_answer`: Whether the LLM's final answer is good or not.\n",
    "- `messages`: The conversation history between the user and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent during the paper research process.\"\"\"\n",
    "    requires_research: bool = False\n",
    "    num_feedback_requests: int = 0\n",
    "    is_good_answer: bool = False\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent tools\n",
    "\n",
    "This cell defines the tools available to the agent. The toolkit contains a tool to search for scientific papers using the CORE API, a tool to download a scientific paper from a given URL, and a tool to ask for human feedback.\n",
    "\n",
    "To make the paper download more robust, the tool includes a retry mechanism, similar to the one used for the CORE API, as well as a mock browser header to avoid 403 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search-papers\", args_schema=SearchPapersInput)\n",
    "def search_papers(query: str, max_papers: int = 1) -> str:\n",
    "    \"\"\"Search for scientific papers using the CORE API.\n",
    "\n",
    "    Example:\n",
    "    {\"query\": \"Attention is all you need\", \"max_papers\": 1}\n",
    "\n",
    "    Returns:\n",
    "        A list of the relevant papers found with the corresponding relevant information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ArxivAPIWrapper(top_k_results=max_papers).search(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error performing paper search: {e}\"\n",
    "\n",
    "@tool(\"download-paper\")\n",
    "def download_paper(url: str) -> str:\n",
    "    \"\"\"Download a specific scientific paper from a given URL.\n",
    "\n",
    "    Example:\n",
    "    {\"url\": \"https://sample.pdf\"}\n",
    "\n",
    "    Returns:\n",
    "        The paper content.\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        http = urllib3.PoolManager(\n",
    "            cert_reqs='CERT_NONE',\n",
    "        )\n",
    "        \n",
    "        # Mock browser headers to avoid 403 error\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            response = http.request('GET', url, headers=headers)\n",
    "            if 200 <= response.status < 300:\n",
    "                pdf_file = io.BytesIO(response.data)\n",
    "                with pdfplumber.open(pdf_file) as pdf:\n",
    "                    text = \"\"\n",
    "                    for page in pdf.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "            elif attempt < max_retries - 1:\n",
    "                time.sleep(2 ** (attempt + 2))\n",
    "            else:\n",
    "                raise Exception(f\"Got non 2xx when downloading paper: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "@tool(\"ask-human-feedback\")\n",
    "def ask_human_feedback(question: str) -> str:\n",
    "    \"\"\"Ask for human feedback. You should call this tool when encountering unexpected errors.\"\"\"\n",
    "    return input(question)\n",
    "\n",
    "tools = [search_papers, download_paper, ask_human_feedback]\n",
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow nodes\n",
    "\n",
    "This cell defines the nodes of the workflow. Note how the `judge_node` is configured to end the execution if the LLM failed to provide a good answer twice to keep latency acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs\n",
    "base_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0, api_key=openai_api_key, base_url=openai_base_url)\n",
    "decision_making_llm = base_llm.with_structured_output(DecisionMakingOutput)\n",
    "agent_llm = base_llm.bind_tools(tools)\n",
    "judge_llm = base_llm.with_structured_output(JudgeOutput)\n",
    "\n",
    "# Decision making node\n",
    "def decision_making_node(state: AgentState):\n",
    "    \"\"\"Entry point of the workflow. Based on the user query, the model can either respond directly or perform a full research, routing the workflow to the planning node\"\"\"\n",
    "    system_prompt = SystemMessage(content=decision_making_prompt)\n",
    "    response: DecisionMakingOutput = decision_making_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\"requires_research\": response.requires_research}\n",
    "    if response.answer:\n",
    "        output[\"messages\"] = [AIMessage(content=response.answer)]\n",
    "    return output\n",
    "\n",
    "# Task router function\n",
    "def router(state: AgentState):\n",
    "    \"\"\"Router directing the user query to the appropriate branch of the workflow.\"\"\"\n",
    "    if state[\"requires_research\"]:\n",
    "        return \"planning\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Planning node\n",
    "def planning_node(state: AgentState):\n",
    "    \"\"\"Planning node that creates a step by step plan to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=planning_prompt.format(tools=format_tools_description(tools)))\n",
    "    response = base_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Tool call node\n",
    "def tools_node(state: AgentState):\n",
    "    \"\"\"Tool call node that executes the tools based on the plan.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_dict[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# Agent call node\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Agent call node that uses the LLM with tools to answer the user query.\"\"\"\n",
    "    system_prompt = SystemMessage(content=arxiv_agent_prompt)\n",
    "    response = agent_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Should continue function\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the agent should continue or end.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # End execution if there are no tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Judge node\n",
    "def judge_node(state: AgentState):\n",
    "    \"\"\"Node to let the LLM judge the quality of its own final answer.\"\"\"\n",
    "    # End execution if the LLM failed to provide a good answer twice.\n",
    "    num_feedback_requests = state.get(\"num_feedback_requests\", 0)\n",
    "    if num_feedback_requests >= 2:\n",
    "        return {\"is_good_answer\": True}\n",
    "\n",
    "    system_prompt = SystemMessage(content=judge_prompt)\n",
    "    response: JudgeOutput = judge_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    output = {\n",
    "        \"is_good_answer\": response.is_good_answer,\n",
    "        \"num_feedback_requests\": num_feedback_requests + 1\n",
    "    }\n",
    "    if response.feedback:\n",
    "        output[\"messages\"] = [AIMessage(content=response.feedback)]\n",
    "    return output\n",
    "\n",
    "# Final answer router function\n",
    "def final_answer_router(state: AgentState):\n",
    "    \"\"\"Router to end the workflow or improve the answer.\"\"\"\n",
    "    if state[\"is_good_answer\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"planning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow definition\n",
    "\n",
    "This cell defines the workflow using LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"decision_making\", decision_making_node)\n",
    "workflow.add_node(\"planning\", planning_node)\n",
    "workflow.add_node(\"tools\", tools_node)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"judge\", judge_node)\n",
    "\n",
    "# Set the entry point of the graph\n",
    "workflow.set_entry_point(\"decision_making\")\n",
    "\n",
    "# Add edges between nodes\n",
    "workflow.add_conditional_edges(\n",
    "    \"decision_making\",\n",
    "    router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"planning\", \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": \"judge\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"judge\",\n",
    "    final_answer_router,\n",
    "    {\n",
    "        \"planning\": \"planning\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usecase for PhD academic research\n",
    "\n",
    "This cell tests the workflow with several example queries. These queries are designed to evaluate the agent on the following aspects:\n",
    "- Completing tasks that are representative of the work a PhD researcher might need to perform.\n",
    "- Addressing more specific tasks that require researching papers within a defined timeframe.\n",
    "- Tackling tasks across multiple areas of research.\n",
    "- Critically evaluating its own responses by sourcing specific information from the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Find 5 recent papers on LLM, agents and reinforcement learning (RL).\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Let's search for 5 recent papers on the topic of \"LLM, agents, and reinforcement learning (RL).\" I will perform the search now.\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search-papers (call_pZv0TvhcbICj8EvS0jJA67oS)\n",
      " Call ID: call_pZv0TvhcbICj8EvS0jJA67oS\n",
      "  Args:\n",
      "    query: LLM AND agents AND reinforcement learning\n",
      "    max_papers: 5\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search-papers\n",
      "\n",
      "\"* Title: Towards Autonomous Reinforcement Learning for Real-World Robotic\\n  Manipulation with Large Language Models\\n* Published Date: 2025-03-06T10:08:44Z\\n* Authors: Niccol\\u00f2 Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco\\n* Abstract: Recent advancements in Large Language Models (LLMs) and Visual Language\\nModels (VLMs) have significantly impacted robotics, enabling high-level\\nsemantic motion planning applications. Reinforcement Learning (RL), a\\ncomplementary paradigm, enables agents to autonomously optimize complex\\nbehaviors through interaction and reward signals. However, designing effective\\nreward functions for RL remains challenging, especially in real-world tasks\\nwhere sparse rewards are insufficient and dense rewards require elaborate\\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\\na pre-trained LLM, to generate reward functions directly from natural language\\ntask descriptions. The rewards are used to train RL agents in simulated\\nenvironments, where we formalize the reward generation process to enhance\\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\\ncreating a fully automated, one-shot procedure for translating human-readable\\ntext into deployable robot skills. Our approach is validated through extensive\\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\\nTasks are demonstrated on the real robot setup.\\n* Link: http://arxiv.org/abs/2503.04280v1\\n\\n-----\\n* Title: Pretrained LLMs as Real-Time Controllers for Robot Operated Serial\\n  Production Line\\n* Published Date: 2025-03-05T20:43:49Z\\n* Authors: Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang\\n* Abstract: The manufacturing industry is undergoing a transformative shift, driven by\\ncutting-edge technologies like 5G, AI, and cloud computing. Despite these\\nadvancements, effective system control, which is crucial for optimizing\\nproduction efficiency, remains a complex challenge due to the intricate,\\nknowledge-dependent nature of manufacturing processes and the reliance on\\ndomain-specific expertise. Conventional control methods often demand heavy\\ncustomization, considerable computational resources, and lack transparency in\\ndecision-making. In this work, we investigate the feasibility of using Large\\nLanguage Models (LLMs), particularly GPT-4, as a straightforward, adaptable\\nsolution for controlling manufacturing systems, specifically, mobile robot\\nscheduling. We introduce an LLM-based control framework to assign mobile robots\\nto different machines in robot assisted serial production lines, evaluating its\\nperformance in terms of system throughput. Our proposed framework outperforms\\ntraditional scheduling approaches such as First-Come-First-Served (FCFS),\\nShortest Processing Time (SPT), and Longest Processing Time (LPT). While it\\nachieves performance that is on par with state-of-the-art methods like\\nMulti-Agent Reinforcement Learning (MARL), it offers a distinct advantage by\\ndelivering comparable throughput without the need for extensive retraining.\\nThese results suggest that the proposed LLM-based solution is well-suited for\\nscenarios where technical expertise, computational resources, and financial\\ninvestment are limited, while decision transparency and system scalability are\\ncritical concerns.\\n* Link: http://arxiv.org/abs/2503.03889v1\\n\\n-----\\n* Title: Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent\\n  Reinforcement Learning in USV Swarm\\n* Published Date: 2025-03-05T14:33:18Z\\n* Authors: Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park\\n* Abstract: Multi-Agent Reinforcement Learning (MARL) has shown promise in solving\\ncomplex problems involving cooperation and competition among agents, such as an\\nUnmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,\\nand vessel protection. However, aligning system behavior with user preferences\\nis challenging due to the difficulty of encoding expert intuition into reward\\nfunctions. To address the issue, we propose a Reinforcement Learning with Human\\nFeedback (RLHF) approach for MARL that resolves credit-assignment challenges\\nthrough an Agent-Level Feedback system categorizing feedback into intra-agent,\\ninter-agent, and intra-team types. To overcome the challenges of direct human\\nfeedback, we employ a Large Language Model (LLM) evaluator to validate our\\napproach using feedback scenarios such as region constraints, collision\\navoidance, and task allocation. Our method effectively refines USV swarm\\npolicies, addressing key challenges in multi-agent systems while maintaining\\nfairness and performance consistency.\\n* Link: http://arxiv.org/abs/2503.03796v1\\n\\n-----\\n* Title: Persuasion at Play: Understanding Misinformation Dynamics in\\n  Demographic-Aware Human-LLM Interactions\\n* Published Date: 2025-03-03T20:30:22Z\\n* Authors: Angana Borah, Rada Mihalcea, Ver\\u00f3nica P\\u00e9rez-Rosas\\n* Abstract: Existing challenges in misinformation exposure and susceptibility vary across\\ndemographic groups, as some populations are more vulnerable to misinformation\\nthan others. Large language models (LLMs) introduce new dimensions to these\\nchallenges through their ability to generate persuasive content at scale and\\nreinforcing existing biases. This study investigates the bidirectional\\npersuasion dynamics between LLMs and humans when exposed to misinformative\\ncontent. We analyze human-to-LLM influence using human-stance datasets and\\nassess LLM-to-human influence by generating LLM-based persuasive arguments.\\nAdditionally, we use a multi-agent LLM framework to analyze the spread of\\nmisinformation under persuasion among demographic-oriented LLM agents. Our\\nfindings show that demographic factors influence susceptibility to\\nmisinformation in LLMs, closely reflecting the demographic-based patterns seen\\nin human susceptibility. We also find that, similar to human demographic\\ngroups, multi-agent LLMs exhibit echo chamber behavior. This research explores\\nthe interplay between humans and LLMs, highlighting demographic differences in\\nthe context of misinformation and offering insights for future interventions.\\n* Link: http://arxiv.org/abs/2503.02038v1\\n\\n-----\\n* Title: Improving Retrospective Language Agents via Joint Policy Gradient\\n  Optimization\\n* Published Date: 2025-03-03T12:54:54Z\\n* Authors: Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, Ji-Rong Wen\\n* Abstract: In recent research advancements within the community, large language models\\n(LLMs) have sparked great interest in creating autonomous agents. However,\\ncurrent prompt-based agents often heavily rely on large-scale LLMs. Meanwhile,\\nalthough fine-tuning methods significantly enhance the capabilities of smaller\\nLLMs, the fine-tuned agents often lack the potential for self-reflection and\\nself-improvement. To address these challenges, we introduce a novel agent\\nframework named RetroAct, which is a framework that jointly optimizes both\\ntask-planning and self-reflective evolution capabilities in language agents.\\nSpecifically, we develop a two-stage joint optimization process that integrates\\nimitation learning and reinforcement learning, and design an off-policy joint\\npolicy gradient optimization algorithm with imitation learning regularization\\nto enhance the data efficiency and training stability in agent tasks. RetroAct\\nsignificantly improves the performance of open-source models, reduces\\ndependency on closed-source LLMs, and enables fine-tuned agents to learn and\\nevolve continuously. We conduct extensive experiments across various testing\\nenvironments, demonstrating RetroAct has substantial improvements in task\\nperformance and decision-making processes.\\n* Link: http://arxiv.org/abs/2503.01490v1\\n\"\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are 5 recent papers on the topic of \"LLM, agents, and reinforcement learning (RL)\":\n",
      "\n",
      "1. **Title:** Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models  \n",
      "   **Authors:** Niccol√≤ Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco  \n",
      "   **Published Date:** March 6, 2025  \n",
      "   **Abstract:** This paper introduces ARCHIE, an unsupervised pipeline leveraging GPT-4 to generate reward functions from natural language task descriptions for RL agents. The approach automates task success criteria coding and demonstrates its effectiveness in robotic manipulation tasks.  \n",
      "   **Link:** [Read the paper](http://arxiv.org/abs/2503.04280v1)  \n",
      "\n",
      "2. **Title:** Pretrained LLMs as Real-Time Controllers for Robot Operated Serial Production Line  \n",
      "   **Authors:** Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang  \n",
      "   **Published Date:** March 5, 2025  \n",
      "   **Abstract:** This study explores the use of GPT-4 as a control framework for mobile robot scheduling in manufacturing systems. The LLM-based framework achieves performance comparable to state-of-the-art methods like Multi-Agent Reinforcement Learning (MARL) without extensive retraining.  \n",
      "   **Link:** [Read the paper](http://arxiv.org/abs/2503.03889v1)  \n",
      "\n",
      "3. **Title:** Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm  \n",
      "   **Authors:** Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park  \n",
      "   **Published Date:** March 5, 2025  \n",
      "   **Abstract:** This paper proposes a Reinforcement Learning with Human Feedback (RLHF) approach for multi-agent systems, using LLMs to validate feedback scenarios. The method refines policies for Unmanned Surface Vehicle (USV) swarms, addressing challenges like collision avoidance and task allocation.  \n",
      "   **Link:** [Read the paper](http://arxiv.org/abs/2503.03796v1)  \n",
      "\n",
      "4. **Title:** Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions  \n",
      "   **Authors:** Angana Borah, Rada Mihalcea, Ver√≥nica P√©rez-Rosas  \n",
      "   **Published Date:** March 3, 2025  \n",
      "   **Abstract:** This research investigates the dynamics of misinformation in human-LLM interactions, using a multi-agent LLM framework to analyze demographic-based susceptibility to misinformation and echo chamber behavior.  \n",
      "   **Link:** [Read the paper](http://arxiv.org/abs/2503.02038v1)  \n",
      "\n",
      "5. **Title:** Improving Retrospective Language Agents via Joint Policy Gradient Optimization  \n",
      "   **Authors:** Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, Ji-Rong Wen  \n",
      "   **Published Date:** March 3, 2025  \n",
      "   **Abstract:** The paper introduces RetroAct, a framework that integrates imitation learning and reinforcement learning to enhance task-planning and self-reflective evolution in language agents. It demonstrates significant improvements in task performance and decision-making.  \n",
      "   **Link:** [Read the paper](http://arxiv.org/abs/2503.01490v1)  \n",
      "\n",
      "Let me know if you'd like to explore any of these papers in more detail!\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## New research running"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Input:\n",
       "\n",
       "Find 5 recent papers on LLM, agents and reasoning.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stream:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\"query\": \"LLM agents reasoning\", \"max_papers\": 5}\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search-papers (call_dL6EAIMJoN66e2ntRsld2g8V)\n",
      " Call ID: call_dL6EAIMJoN66e2ntRsld2g8V\n",
      "  Args:\n",
      "    query: LLM agents reasoning\n",
      "    max_papers: 5\n",
      "\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search-papers\n",
      "\n",
      "\"* Title: LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM\\n* Published Date: 2025-03-06T18:59:38Z\\n* Authors: Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jean Lahoud, Fahad Khan, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal\\n* Abstract: Recent advancements in speech-to-speech dialogue systems leverage LLMs for\\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\\nhigh computational overhead, and text-speech misalignment. Existing\\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\\nthereby compromising its linguistic capabilities. In contrast, we propose\\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\\nsystem that generates high-quality speech with low latency, while fully\\npreserving the capabilities of the base LLM. Our approach achieves a\\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\\nextension to various tasks with different backbones. Furthermore, LLMVoX\\ngeneralizes to new languages with only dataset adaptation, attaining a low\\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\\nand vision capabilities, without requiring additional multimodal training. Our\\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .\\n* Link: http://arxiv.org/abs/2503.04724v1\\n\\n-----\\n* Title: Shifting Long-Context LLMs Research from Input to Output\\n* Published Date: 2025-03-06T18:59:37Z\\n* Authors: Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee\\n* Abstract: Recent advancements in long-context Large Language Models (LLMs) have\\nprimarily concentrated on processing extended input contexts, resulting in\\nsignificant strides in long-context comprehension. However, the equally\\ncritical aspect of generating long-form outputs has received comparatively less\\nattention. This paper advocates for a paradigm shift in NLP research toward\\naddressing the challenges of long-output generation. Tasks such as novel\\nwriting, long-term planning, and complex reasoning require models to understand\\nextensive contexts and produce coherent, contextually rich, and logically\\nconsistent extended text. These demands highlight a critical gap in current LLM\\ncapabilities. We underscore the importance of this under-explored domain and\\ncall for focused efforts to develop foundational LLMs tailored for generating\\nhigh-quality, long-form outputs, which hold immense potential for real-world\\napplications.\\n* Link: http://arxiv.org/abs/2503.04723v1\\n\\n-----\\n* Title: Enough Coin Flips Can Make LLMs Act Bayesian\\n* Published Date: 2025-03-06T18:59:23Z\\n* Authors: Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, Eric Wang, Dan Klein, Trevor Darrell, David M. Chan\\n* Abstract: Large language models (LLMs) exhibit the ability to generalize given few-shot\\nexamples in their input prompt, an emergent capability known as in-context\\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\\nreasoning in ways that are consistent with a Bayesian framework or rely on\\npattern matching. Using a controlled setting of biased coin flips, we find\\nthat: (1) LLMs often possess biased priors, causing initial divergence in\\nzero-shot settings, (2) in-context evidence outweighs explicit bias\\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\\ndeviations primarily due to miscalibrated priors rather than flawed updates,\\nand (4) attention magnitude has negligible effect on Bayesian inference. With\\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\\npriors in a Bayesian manner.\\n* Link: http://arxiv.org/abs/2503.04722v1\\n\\n-----\\n* Title: Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\\n  Language Model Pretraining\\n* Published Date: 2025-03-06T18:58:29Z\\n* Authors: Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Yangshijie Xu, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang\\n* Abstract: The impressive capabilities of Large Language Models (LLMs) across diverse\\ntasks are now well-established, yet their effective deployment necessitates\\ncareful hyperparameter optimization. Through extensive empirical studies\\ninvolving grid searches across diverse configurations, we discover universal\\nscaling laws governing these hyperparameters: optimal learning rate follows a\\npower-law relationship with both model parameters and data sizes, while optimal\\nbatch size scales primarily with data sizes. Our analysis reveals a convex\\noptimization landscape for hyperparameters under fixed models and data size\\nconditions. This convexity implies an optimal hyperparameter plateau. We\\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\\ncommunity. Its estimated values on the test set are merely 0.07\\\\% away from the\\nglobally optimal LLM performance found via an exhaustive search. These laws\\ndemonstrate remarkable robustness across variations in model sparsity, training\\ndata distribution, and model shape. To our best known, this is the first work\\nthat unifies different model shapes and structures, such as Mixture-of-Experts\\nmodels and dense transformers, as well as establishes optimal hyperparameter\\nscaling laws across diverse data distributions. This exhaustive optimization\\nprocess demands substantial computational resources, utilizing nearly one\\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\\ntotal. To facilitate reproducibility and further research, we will\\nprogressively release all loss measurements and model checkpoints through our\\ndesignated repository https://step-law.github.io/\\n* Link: http://arxiv.org/abs/2503.04715v1\\n\\n-----\\n* Title: Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\\n  Architecture and Size\\n* Published Date: 2025-03-06T18:54:32Z\\n* Authors: Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison\\n* Abstract: We present a novel approach to selective model quantization that transcends\\nthe limitations of architecture-specific and size-dependent compression methods\\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\\nanalyzing the entropy distribution across transformer blocks, EWQ determines\\nwhich blocks can be safely quantized without causing significant performance\\ndegradation, independent of model architecture or size. Our method outperforms\\nuniform quantization approaches, maintaining Massive Multitask Language\\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\\nimprovements in the quality-compression trade-off regardless of model scale or\\narchitectural design. A surprising finding of EWQ is its ability to reduce\\nperplexity compared to unquantized models, suggesting the presence of\\nbeneficial regularization through selective precision reduction. This\\nimprovement holds across different model families, indicating a fundamental\\nrelationship between layer-level entropy and optimal precision requirements.\\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\\nanalysis that eliminates the need for loading model weights. This technique\\nleverages universal characteristics of entropy distribution that persist across\\nvarious architectures and scales, enabling near-instantaneous quantization\\ndecisions while maintaining 80% classification accuracy with full entropy\\nanalysis. Our results demonstrate that effective quantization strategies can be\\ndeveloped independently of specific architectural choices or model sizes,\\nopening new possibilities for efficient LLM deployment.\\n* Link: http://arxiv.org/abs/2503.04704v1\\n\"\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are 5 recent papers on LLMs, agents, and reasoning:\n",
      "\n",
      "1. **Title:** [LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM](http://arxiv.org/abs/2503.04724v1)  \n",
      "   **Authors:** Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, et al.  \n",
      "   **Abstract:** This paper introduces LLMVoX, a lightweight, LLM-agnostic, autoregressive streaming text-to-speech system. It focuses on improving multimodal interactions in speech-to-speech dialogue systems by decoupling speech synthesis from LLM processing. The system supports infinite-length dialogues and generalizes to new languages with minimal adaptation.  \n",
      "   **Published Date:** 2025-03-06  \n",
      "\n",
      "2. **Title:** [Shifting Long-Context LLMs Research from Input to Output](http://arxiv.org/abs/2503.04723v1)  \n",
      "   **Authors:** Yuhao Wu, Yushi Bai, Zhiqing Hu, et al.  \n",
      "   **Abstract:** This paper advocates for a shift in research focus from processing extended input contexts to generating long-form outputs. It highlights the challenges in tasks like novel writing, long-term planning, and complex reasoning, emphasizing the need for foundational LLMs tailored for high-quality, long-form text generation.  \n",
      "   **Published Date:** 2025-03-06  \n",
      "\n",
      "3. **Title:** [Enough Coin Flips Can Make LLMs Act Bayesian](http://arxiv.org/abs/2503.04722v1)  \n",
      "   **Authors:** Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, et al.  \n",
      "   **Abstract:** This study explores whether LLMs perform structured reasoning consistent with Bayesian frameworks. Using biased coin flips as a controlled setting, the authors find that LLMs can update their priors in a Bayesian manner with sufficient in-context learning demonstrations.  \n",
      "   **Published Date:** 2025-03-06  \n",
      "\n",
      "4. **Title:** [Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining](http://arxiv.org/abs/2503.04715v1)  \n",
      "   **Authors:** Houyi Li, Wenzheng Zheng, Jingcheng Hu, et al.  \n",
      "   **Abstract:** This paper identifies universal scaling laws for hyperparameters in LLM pretraining. It provides insights into optimal learning rates and batch sizes, offering a plug-and-play tool for hyperparameter optimization. The findings are validated across diverse model architectures and data distributions.  \n",
      "   **Published Date:** 2025-03-06  \n",
      "\n",
      "5. **Title:** [Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size](http://arxiv.org/abs/2503.04704v1)  \n",
      "   **Authors:** Alireza Behtash, Marijan Fofonjka, Ethan Baird, et al.  \n",
      "   **Abstract:** This paper presents Entropy-Weighted Quantization (EWQ), a method for selective model quantization that is independent of model architecture or size. The approach reduces memory usage while maintaining performance, offering a new perspective on efficient LLM deployment.  \n",
      "   **Published Date:** 2025-03-06  \n",
      "\n",
      "Let me know if you'd like to explore any of these papers in more detail!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    \"Find 5 recent papers on LLM, and reinforcement learning (RL).\",\n",
    "    \n",
    "    \"Find 5 recent papers on LLM, and reasoning.\"\n",
    "]\n",
    "\n",
    "# Run tests and store the results for later visualisation\n",
    "outputs = []\n",
    "for test_input in test_inputs:\n",
    "    final_answer = await print_stream(app, test_input)\n",
    "    outputs.append(final_answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results\n",
    "\n",
    "This cell displays the results of the test queries for a more compact visualisation of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Find 5 recent papers on LLM, agents and reinforcement learning (RL).\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are 5 recent papers on the topic of \"LLM, agents, and reinforcement learning (RL)\":\n",
       "\n",
       "1. **Title:** Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models  \n",
       "   **Authors:** Niccol√≤ Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco  \n",
       "   **Published Date:** March 6, 2025  \n",
       "   **Abstract:** This paper introduces ARCHIE, an unsupervised pipeline leveraging GPT-4 to generate reward functions from natural language task descriptions for RL agents. The approach automates task success criteria coding and demonstrates its effectiveness in robotic manipulation tasks.  \n",
       "   **Link:** [Read the paper](http://arxiv.org/abs/2503.04280v1)  \n",
       "\n",
       "2. **Title:** Pretrained LLMs as Real-Time Controllers for Robot Operated Serial Production Line  \n",
       "   **Authors:** Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang  \n",
       "   **Published Date:** March 5, 2025  \n",
       "   **Abstract:** This study explores the use of GPT-4 as a control framework for mobile robot scheduling in manufacturing systems. The LLM-based framework achieves performance comparable to state-of-the-art methods like Multi-Agent Reinforcement Learning (MARL) without extensive retraining.  \n",
       "   **Link:** [Read the paper](http://arxiv.org/abs/2503.03889v1)  \n",
       "\n",
       "3. **Title:** Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm  \n",
       "   **Authors:** Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park  \n",
       "   **Published Date:** March 5, 2025  \n",
       "   **Abstract:** This paper proposes a Reinforcement Learning with Human Feedback (RLHF) approach for multi-agent systems, using LLMs to validate feedback scenarios. The method refines policies for Unmanned Surface Vehicle (USV) swarms, addressing challenges like collision avoidance and task allocation.  \n",
       "   **Link:** [Read the paper](http://arxiv.org/abs/2503.03796v1)  \n",
       "\n",
       "4. **Title:** Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions  \n",
       "   **Authors:** Angana Borah, Rada Mihalcea, Ver√≥nica P√©rez-Rosas  \n",
       "   **Published Date:** March 3, 2025  \n",
       "   **Abstract:** This research investigates the dynamics of misinformation in human-LLM interactions, using a multi-agent LLM framework to analyze demographic-based susceptibility to misinformation and echo chamber behavior.  \n",
       "   **Link:** [Read the paper](http://arxiv.org/abs/2503.02038v1)  \n",
       "\n",
       "5. **Title:** Improving Retrospective Language Agents via Joint Policy Gradient Optimization  \n",
       "   **Authors:** Xueyang Feng, Bo Lan, Quanyu Dai, Lei Wang, Jiakai Tang, Xu Chen, Zhenhua Dong, Ji-Rong Wen  \n",
       "   **Published Date:** March 3, 2025  \n",
       "   **Abstract:** The paper introduces RetroAct, a framework that integrates imitation learning and reinforcement learning to enhance task-planning and self-reflective evolution in language agents. It demonstrates significant improvements in task performance and decision-making.  \n",
       "   **Link:** [Read the paper](http://arxiv.org/abs/2503.01490v1)  \n",
       "\n",
       "Let me know if you'd like to explore any of these papers in more detail!\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Input:\n",
       "\n",
       "Find 5 recent papers on LLM, agents and reasoning.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Output:\n",
       "\n",
       "Here are 5 recent papers on LLMs, agents, and reasoning:\n",
       "\n",
       "1. **Title:** [LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM](http://arxiv.org/abs/2503.04724v1)  \n",
       "   **Authors:** Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, et al.  \n",
       "   **Abstract:** This paper introduces LLMVoX, a lightweight, LLM-agnostic, autoregressive streaming text-to-speech system. It focuses on improving multimodal interactions in speech-to-speech dialogue systems by decoupling speech synthesis from LLM processing. The system supports infinite-length dialogues and generalizes to new languages with minimal adaptation.  \n",
       "   **Published Date:** 2025-03-06  \n",
       "\n",
       "2. **Title:** [Shifting Long-Context LLMs Research from Input to Output](http://arxiv.org/abs/2503.04723v1)  \n",
       "   **Authors:** Yuhao Wu, Yushi Bai, Zhiqing Hu, et al.  \n",
       "   **Abstract:** This paper advocates for a shift in research focus from processing extended input contexts to generating long-form outputs. It highlights the challenges in tasks like novel writing, long-term planning, and complex reasoning, emphasizing the need for foundational LLMs tailored for high-quality, long-form text generation.  \n",
       "   **Published Date:** 2025-03-06  \n",
       "\n",
       "3. **Title:** [Enough Coin Flips Can Make LLMs Act Bayesian](http://arxiv.org/abs/2503.04722v1)  \n",
       "   **Authors:** Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, et al.  \n",
       "   **Abstract:** This study explores whether LLMs perform structured reasoning consistent with Bayesian frameworks. Using biased coin flips as a controlled setting, the authors find that LLMs can update their priors in a Bayesian manner with sufficient in-context learning demonstrations.  \n",
       "   **Published Date:** 2025-03-06  \n",
       "\n",
       "4. **Title:** [Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining](http://arxiv.org/abs/2503.04715v1)  \n",
       "   **Authors:** Houyi Li, Wenzheng Zheng, Jingcheng Hu, et al.  \n",
       "   **Abstract:** This paper identifies universal scaling laws for hyperparameters in LLM pretraining. It provides insights into optimal learning rates and batch sizes, offering a plug-and-play tool for hyperparameter optimization. The findings are validated across diverse model architectures and data distributions.  \n",
       "   **Published Date:** 2025-03-06  \n",
       "\n",
       "5. **Title:** [Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size](http://arxiv.org/abs/2503.04704v1)  \n",
       "   **Authors:** Alireza Behtash, Marijan Fofonjka, Ethan Baird, et al.  \n",
       "   **Abstract:** This paper presents Entropy-Weighted Quantization (EWQ), a method for selective model quantization that is independent of model architecture or size. The approach reduces memory usage while maintaining performance, offering a new perspective on efficient LLM deployment.  \n",
       "   **Published Date:** 2025-03-06  \n",
       "\n",
       "Let me know if you'd like to explore any of these papers in more detail!\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input, output in zip(test_inputs, outputs):\n",
    "    display(Markdown(f\"## Input:\\n\\n{input}\\n\\n\"))\n",
    "    display(Markdown(f\"## Output:\\n\\n{output}\\n\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "In this comprehensive analysis, we evaluated our Scientific Paper Agent against two leading AI knowledge co pilots : Microsoft Copilot and Perplexity AI. Using a standardized query - \"Find 8 papers on quantum machine learning\" - we conducted a detailed comparison across multiple dimensions to understand the strengths, limitations, and optimal use cases for each system.\n",
    "\n",
    "\n",
    "#### Test Case Implementation\n",
    "\n",
    "We implemented a controlled test using the same research query across all three platforms:\n",
    "- Query: \"Find 8 papers on quantum machine learning\"\n",
    "- Sample Size: Multiple test runs to ensure consistency\n",
    "- Evaluation Time: Early 2024\n",
    "- Metrics Tracked: Response time, metadata quality, and result structure\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "While our agent demonstrated superior academic rigor and metadata completeness, taking approximately 30 seconds per query, competitors like Microsoft Copilot (2 seconds) and Perplexity AI (4-5 seconds) showed advantages in response speed. This tradeoff between speed and depth reflects different design philosophies and target use cases.\n",
    "\n",
    "The comparative analysis reveals a clear differentiation in approaches:\n",
    "- Our Agent: Optimized for thorough academic research with comprehensive validation\n",
    "- Microsoft Copilot: Focused on rapid information retrieval and general overview\n",
    "- Perplexity AI: Balanced approach with emphasis on source verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft  copilot results \n",
    "\n",
    "![image](https://i.ibb.co/y4Zf4Pc/Screenshot-2024-11-17-at-21-40-21.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity AI results\n",
    "![image](https://i.ibb.co/n1rr7kW/Screenshot-2024-11-17-at-21-40-42.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Comparsion\n",
    "![image](https://i.ibb.co/5KbTmFq/Screenshot-2024-11-17-at-22-03-43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present a comprehensive comparison between our research assistant agent and leading platforms (Microsoft Copilot and Perplexity AI). Using a standardized query - \"Find 8 papers on quantum machine learning\" - we evaluated performance across key metrics including response time, metadata quality, and academic value. Our analysis reveals distinct trade-offs: while our agent takes longer to process (30s vs. 2-5s), it provides significantly more detailed metadata, validated sources, and structured academic output. The comparison table above breaks down these differences across multiple dimensions, helping users choose the right tool for their specific research needs - whether it's quick exploration (where Copilot excels) or deep academic research (where our agent shows its strengths)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Limitations\n",
    "\n",
    "1. Technical Limitations\n",
    "    - API rate limits for paper access\n",
    "    - Handle time for large PDFs\n",
    "    - Limited to publicly accessible papers\n",
    "  \n",
    "2. Functional Limitations\n",
    "    - No support for image analysis in papers\n",
    "    - Limited context window for very long papers\n",
    "    - Cannot perform mathematical derivations\n",
    "    - Language constraints for non-English papers\n",
    "\n",
    "\n",
    "## Potential Improvements:\n",
    "\n",
    "1. Technical Improvements\n",
    "    - Implement parallel processing for multiple papers\n",
    "    - Add caching system for frequently accessed papers\n",
    "    - Integrate multiple academic APIs for broader coverage\n",
    "    - Implement batch processing for large datasets\n",
    "\n",
    "2. Functional Improvements\n",
    "    - Add support for figure and table extraction\n",
    "    - Implement cross-referencing between papers\n",
    "    - Add citation network analysis\n",
    "    - Include domain-specific validation rules\n",
    "        \n",
    "3. User Experience\n",
    "    - Add interactive feedback mechanisms\n",
    "    - Implement progress tracking\n",
    "    - Add customizable validation criteria\n",
    "    - Include export options for research summaries\n",
    "        \n",
    "   \n",
    "## Specific Use Cases:\n",
    "\n",
    "1. Academic Research, Literature review and paper analysis.\n",
    "    - Comprehensive search\n",
    "    - Citation tracking\n",
    "    - Cross-reference validation\n",
    "\n",
    "2. Industry Research, Technical documentation and patent analysis.\n",
    "    - Focused search\n",
    "    - Technical specification extraction\n",
    "    - Competitive analysis\n",
    "\n",
    "3. Educational, Student research assistance.\n",
    "    - Simplified explanations\n",
    "    - Learning resource identification\n",
    "    - Guided research process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This implementation demonstrates how state-driven architectures can transform academic paper analysis. By combining LangGraph's orchestration capabilities with robust API integrations, we've created a system that maintains research rigor while automating key aspects of paper processing. The workflow's emphasis on validation and quality control ensures reliable research outputs while significantly streamlining the paper analysis process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
